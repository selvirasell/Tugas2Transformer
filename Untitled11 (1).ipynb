{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Optional: visualization\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    MATPLOTLIB_AVAILABLE = True\n",
        "except Exception:\n",
        "    MATPLOTLIB_AVAILABLE = False\n",
        "\n",
        "# --------------------------- Utilities ---------------------------------\n",
        "def softmax(x, axis=-1):\n",
        "    # numerically stable softmax\n",
        "    x_max = np.max(x, axis=axis, keepdims=True)\n",
        "    e_x = np.exp(x - x_max)\n",
        "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
        "\n",
        "def gelu(x):\n",
        "    # GELU approximation\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "# --------------------------- LayerNorm --------------------------------\n",
        "class LayerNorm:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "        self.gamma = np.ones((dim,))\n",
        "        self.beta = np.zeros((dim,))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: [..., dim]\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        var = np.var(x, axis=-1, keepdims=True)\n",
        "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "# --------------------------- Embedding --------------------------------\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        # initialize embedding matrix: (vocab, d_model)\n",
        "        self.W = np.random.randn(vocab_size, d_model) / np.sqrt(d_model)\n",
        "\n",
        "    def __call__(self, token_ids):\n",
        "        # token_ids: [batch, seq_len]\n",
        "        return self.W[token_ids]  # returns [batch, seq_len, d_model]\n",
        "\n",
        "# ------------------------ Positional Encoding --------------------------\n",
        "class PositionalEncoding:\n",
        "    def __init__(self, d_model, max_len=1024, method='sinusoidal'):\n",
        "        \"\"\"\n",
        "        method: 'sinusoidal' or 'rope'\n",
        "        For 'rope', this class will prepare sinusoidal frequencies used by RoPE.\n",
        "        \"\"\"\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.method = method\n",
        "        if method == 'sinusoidal':\n",
        "            pe = np.zeros((max_len, d_model))\n",
        "            position = np.arange(0, max_len)[:, np.newaxis]\n",
        "            div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = np.sin(position * div_term)\n",
        "            pe[:, 1::2] = np.cos(position * div_term)\n",
        "            self.pe = pe  # shape (max_len, d_model)\n",
        "        elif method == 'rope':\n",
        "            # Prepare RoPE frequency terms for rotary embeddings.\n",
        "            # We'll compute angles for each even dimension as in many RoPE implementations.\n",
        "            half = d_model // 2\n",
        "            inv_freq = 1.0 / (10000 ** (np.arange(0, half) / float(half)))\n",
        "            # store inv_freq to compute sin/cos for positions at runtime\n",
        "            self.inv_freq = inv_freq  # shape (half,)\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'sinusoidal' or 'rope'\")\n",
        "\n",
        "    def sinusoidal(self, seq_len):\n",
        "        if seq_len > self.max_len:\n",
        "            raise ValueError(\"seq_len exceeds max_len\")\n",
        "        return self.pe[:seq_len]  # (seq_len, d_model)\n",
        "\n",
        "    def rope_sin_cos(self, seq_len, head_dim):\n",
        "        \"\"\"\n",
        "        For RoPE, compute sin and cos arrays for positions.\n",
        "        Return sin, cos shaped (seq_len, head_dim) where head_dim is half*2 maybe.\n",
        "        We'll create sin/cos for the head_dim dimension.\n",
        "        \"\"\"\n",
        "        # Ensure head_dim is even\n",
        "        if head_dim % 2 != 0:\n",
        "            raise ValueError(\"head_dim must be even for RoPE\")\n",
        "        half = head_dim // 2\n",
        "        pos = np.arange(seq_len)\n",
        "        angles = np.outer(pos, self.inv_freq[:half])  # (seq_len, half)\n",
        "        sin = np.sin(angles)  # (seq_len, half)\n",
        "        cos = np.cos(angles)\n",
        "        # Interleave to match head_dim: [sin0, sin1, ...] and same shape for cos by repeating\n",
        "        sin_big = np.concatenate([sin, sin], axis=-1)  # (seq_len, head_dim)\n",
        "        cos_big = np.concatenate([cos, cos], axis=-1)\n",
        "        return sin_big, cos_big\n",
        "\n",
        "# -------------------- Causal Mask (for attention) ----------------------\n",
        "def causal_mask(seq_len):\n",
        "    # mask shape [seq_len, seq_len], True where masked (future positions)\n",
        "    return np.triu(np.ones((seq_len, seq_len), dtype=bool), k=1)\n",
        "\n",
        "# ----------------- Scaled Dot-Product Attention ------------------------\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    q,k,v: [..., seq_q, head_dim] and [..., seq_k, head_dim]\n",
        "    mask: broadcastable boolean mask where True means masked (should not attend)\n",
        "    \"\"\"\n",
        "    dk = q.shape[-1]\n",
        "    scores = np.matmul(q, np.swapaxes(k, -1, -2)) / np.sqrt(dk)  # [..., seq_q, seq_k]\n",
        "    if mask is not None:\n",
        "        # mask: (seq_q, seq_k) -> expand to broadcasting shape of scores\n",
        "        scores = np.where(mask, -1e9, scores)\n",
        "    attn = softmax(scores, axis=-1)\n",
        "    out = np.matmul(attn, v)\n",
        "    return out, attn\n",
        "\n",
        "# ----------------------- Multi-Head Attention --------------------------\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads, rope=False, max_len=512, posenc_obj=None):\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.rope = rope\n",
        "        self.max_len = max_len\n",
        "        # projection weights\n",
        "        k = 1 / np.sqrt(d_model)\n",
        "        self.W_q = np.random.randn(d_model, d_model) * k\n",
        "        self.W_k = np.random.randn(d_model, d_model) * k\n",
        "        self.W_v = np.random.randn(d_model, d_model) * k\n",
        "        self.W_o = np.random.randn(d_model, d_model) * k\n",
        "        # posenc_obj is PositionalEncoding instance when using RoPE\n",
        "        self.posenc_obj = posenc_obj\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # x: [batch, seq_len, d_model] -> [batch, num_heads, seq_len, head_dim]\n",
        "        b, s, _ = x.shape\n",
        "        x = x.reshape(b, s, self.num_heads, self.head_dim)\n",
        "        return np.transpose(x, (0, 2, 1, 3))\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # x: [batch, num_heads, seq_len, head_dim] -> [batch, seq_len, d_model]\n",
        "        b, h, s, hd = x.shape\n",
        "        x = np.transpose(x, (0, 2, 1, 3)).reshape(b, s, h * hd)\n",
        "        return x\n",
        "\n",
        "    def apply_rope_to_head(self, x_head, sin, cos):\n",
        "        \"\"\"\n",
        "        x_head: (batch, heads, seq_len, head_dim)\n",
        "        sin, cos: (seq_len, head_dim) broadcast to (1,1,seq_len,head_dim)\n",
        "        RoPE rotation: split head_dim into (x1, x2) halves and rotate:\n",
        "        [x1, x2] -> [x1 * cos - x2 * sin, x2 * cos + x1 * sin]\n",
        "        \"\"\"\n",
        "        # ensure head_dim even\n",
        "        hd = x_head.shape[-1]\n",
        "        half = hd // 2\n",
        "        x1 = x_head[..., :half]\n",
        "        x2 = x_head[..., half:]\n",
        "        sin_b = sin[np.newaxis, np.newaxis, :, :half]\n",
        "        cos_b = cos[np.newaxis, np.newaxis, :, :half]\n",
        "        # rotate\n",
        "        x1_rot = x1 * cos_b - x2 * sin_b\n",
        "        x2_rot = x2 * cos_b + x1 * sin_b\n",
        "        return np.concatenate([x1_rot, x2_rot], axis=-1)\n",
        "\n",
        "    def __call__(self, x, mask=None):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        b, seq_len, _ = x.shape\n",
        "        Q = x @ self.W_q  # [b, seq, d_model]\n",
        "        K = x @ self.W_k\n",
        "        V = x @ self.W_v\n",
        "        Qh = self.split_heads(Q)  # [b, h, seq, head_dim]\n",
        "        Kh = self.split_heads(K)\n",
        "        Vh = self.split_heads(V)\n",
        "\n",
        "        # Apply RoPE if requested (rotary positional embedding on Q and K)\n",
        "        if self.rope and self.posenc_obj is not None:\n",
        "            # get sin/cos for seq_len and head_dim\n",
        "            sin_big, cos_big = self.posenc_obj.rope_sin_cos(seq_len, self.head_dim)\n",
        "            # apply to Qh and Kh\n",
        "            Qh = self.apply_rope_to_head(Qh, sin_big, cos_big)\n",
        "            Kh = self.apply_rope_to_head(Kh, sin_big, cos_big)\n",
        "\n",
        "        # prepare mask: (seq_len, seq_len) -> broadcastable to (b, h, seq, seq)\n",
        "        attn_mask = None\n",
        "        if mask is not None:\n",
        "            attn_mask = mask[np.newaxis, np.newaxis, :, :]\n",
        "\n",
        "        out_h, attn = scaled_dot_product_attention(Qh, Kh, Vh, mask=attn_mask)  # out_h: [b,h,seq,head_dim]\n",
        "        out = self.combine_heads(out_h)  # [b, seq, d_model]\n",
        "        out = out @ self.W_o  # final linear\n",
        "        return out, attn\n",
        "\n",
        "# ------------------------- Feed-Forward --------------------------------\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        k1 = 1 / np.sqrt(d_model)\n",
        "        k2 = 1 / np.sqrt(d_ff)\n",
        "        self.W1 = np.random.randn(d_model, d_ff) * k1\n",
        "        self.b1 = np.zeros((d_ff,))\n",
        "        self.W2 = np.random.randn(d_ff, d_model) * k2\n",
        "        self.b2 = np.zeros((d_model,))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: [b, seq, d_model]\n",
        "        x1 = x @ self.W1 + self.b1\n",
        "        a1 = gelu(x1)\n",
        "        x2 = a1 @ self.W2 + self.b2\n",
        "        return x2\n",
        "\n",
        "# ------------------------ Decoder Block (pre-norm) ---------------------\n",
        "class DecoderBlock:\n",
        "    def __init__(self, d_model, num_heads, d_ff, rope=False, posenc_obj=None):\n",
        "        self.ln1 = LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, rope=rope, posenc_obj=posenc_obj)\n",
        "        self.ln2 = LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def __call__(self, x, mask=None):\n",
        "        x_norm = self.ln1(x)\n",
        "        mha_out, attn = self.mha(x_norm, mask=mask)\n",
        "        x = x + mha_out\n",
        "        x_norm2 = self.ln2(x)\n",
        "        ff_out = self.ff(x_norm2)\n",
        "        x = x + ff_out\n",
        "        return x, attn\n",
        "\n",
        "# -------------------------- Transformer Decoder ------------------------\n",
        "class TransformerDecoder:\n",
        "    def __init__(self, vocab_size, d_model=128, max_len=128, num_layers=4, num_heads=4, d_ff=512,\n",
        "                 pos_method='sinusoidal', enable_weight_tying=True):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.token_emb = TokenEmbedding(vocab_size, d_model)\n",
        "        # positional encoding object\n",
        "        self.posenc = PositionalEncoding(d_model, max_len=max_len, method=pos_method)\n",
        "        # decide whether to use RoPE inside MHA\n",
        "        rope_flag = (pos_method == 'rope')\n",
        "        self.layers = [DecoderBlock(d_model, num_heads, d_ff, rope=rope_flag, posenc_obj=self.posenc)\n",
        "                       for _ in range(num_layers)]\n",
        "        self.ln_f = LayerNorm(d_model)\n",
        "        # weight tying: unembedding uses same weights as token_emb.W (transpose)\n",
        "        if enable_weight_tying:\n",
        "            # direct reference to transpose of token embedding weight (no copy)\n",
        "            # Note: using the transpose view for matmul is fine; keep in mind numpy returns a view for .T\n",
        "            self.unembed = self.token_emb.W.T\n",
        "        else:\n",
        "            self.unembed = np.random.randn(d_model, vocab_size) / np.sqrt(d_model)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        # token_ids: [batch, seq_len]\n",
        "        b, seq_len = token_ids.shape\n",
        "        if seq_len > self.max_len:\n",
        "            raise ValueError(\"seq_len exceeds max_len\")\n",
        "\n",
        "        x = self.token_emb(token_ids) * np.sqrt(self.d_model)  # [b, seq, d_model]\n",
        "\n",
        "        if self.posenc.method == 'sinusoidal':\n",
        "            pe = self.posenc.sinusoidal(seq_len)[np.newaxis, :, :]  # [1, seq, d_model]\n",
        "            x = x + pe\n",
        "        else:\n",
        "            # For RoPE, positions are encoded inside attention; still can optionally add small positional bias or skip\n",
        "            # We'll skip additive positional encoding when using RoPE (common practice)\n",
        "            pass\n",
        "\n",
        "        mask = causal_mask(seq_len)  # (seq, seq) boolean True for masked/future\n",
        "        all_attn = []\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, mask=mask)\n",
        "            all_attn.append(attn)\n",
        "        x = self.ln_f(x)\n",
        "        logits = x @ self.unembed  # [b, seq, vocab]\n",
        "        probs = softmax(logits, axis=-1)\n",
        "        return logits, probs, all_attn\n",
        "\n",
        "# ------------------------------- Demo / Test ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Configuration (small for demo)\n",
        "    VOCAB = 1000\n",
        "    BATCH = 2\n",
        "    SEQ_LEN = 10\n",
        "    D_MODEL = 64\n",
        "    MAX_LEN = 128\n",
        "    NUM_LAYERS = 3\n",
        "    NUM_HEADS = 4\n",
        "    D_FF = 256\n",
        "\n",
        "    # Choose positional encoding method: 'sinusoidal' or 'rope'\n",
        "    POS_METHOD = 'sinusoidal'  # change to 'rope' to enable RoPE\n",
        "\n",
        "    model = TransformerDecoder(vocab_size=VOCAB,\n",
        "                               d_model=D_MODEL,\n",
        "                               max_len=MAX_LEN,\n",
        "                               num_layers=NUM_LAYERS,\n",
        "                               num_heads=NUM_HEADS,\n",
        "                               d_ff=D_FF,\n",
        "                               pos_method=POS_METHOD,\n",
        "                               enable_weight_tying=True)\n",
        "\n",
        "    # Dummy input tokens\n",
        "    tokens = np.random.randint(0, VOCAB, size=(BATCH, SEQ_LEN))\n",
        "    print(\"Input token ids shape:\", tokens.shape)\n",
        "\n",
        "    logits, probs, attn_maps = model.forward(tokens)\n",
        "    print(\"Logits shape:\", logits.shape)             # [batch, seq, vocab]\n",
        "    print(\"Probabilities for last token shape:\", probs[:, -1, :].shape)  # [batch, vocab]\n",
        "\n",
        "    # basic checks and information\n",
        "    print(\"Sample logits (batch0, last pos, first 10):\", logits[0, -1, :10])\n",
        "    print(\"Sum of prob distribution (batch0, last pos):\", np.sum(probs[0, -1, :]))\n",
        "    last_attn = attn_maps[-1]  # shape [batch, heads, seq, seq]\n",
        "    print(\"Last-attention shape:\", last_attn.shape)\n",
        "\n",
        "    # top-k for last position of batch 0\n",
        "    top5_idx = np.argsort(probs[0, -1, :])[-5:][::-1]\n",
        "    print(\"Top-5 next-token predictions (batch0, last pos):\", top5_idx)\n",
        "\n",
        "    # show example attention row (position 2) for head 0, batch 0\n",
        "    sample_attn = last_attn[0, 0]  # (seq, seq)\n",
        "    print(\"Attention row for position 2 (example):\", np.round(sample_attn[2], 6))\n",
        "\n",
        "    # Verify weight tying behavior (optional check)\n",
        "    # If weight tying enabled, logits[:, -1, :] computed with unembed (token_emb.W.T) should be same as manual matmul\n",
        "    if model.unembed is model.token_emb.W.T:\n",
        "        # quick numerical check for first element at last position (batch0)\n",
        "        manual = logits[0, -1, :]\n",
        "        tied = ( ( ( (model.token_emb.W).T @ ( (model.ln_f(np.zeros((1,D_MODEL))) ).T ) ) ) ) if False else None\n",
        "        # Above manual check is non-trivial; skip strict eq test; instead just inform it's tied by reference\n",
        "        print(\"Weight tying enabled: unembed is a transpose view of token embedding matrix.\")\n",
        "\n",
        "    # Visualize attention map from last layer, head 0, batch 0\n",
        "    if MATPLOTLIB_AVAILABLE:\n",
        "        att_map = last_attn[0, 0]  # (seq, seq)\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(att_map, aspect='auto', cmap='viridis')\n",
        "        plt.title(f\"Attention Map (last layer, head 0) - pos_method={POS_METHOD}\")\n",
        "        plt.xlabel(\"Key positions\")\n",
        "        plt.ylabel(\"Query positions\")\n",
        "        out_path = \"attention_map.png\"\n",
        "        plt.savefig(out_path, dpi=150)\n",
        "        plt.show()   # tambahkan ini untuk langsung tampilkan gambarnya\n",
        "\n",
        "        plt.close()\n",
        "        print(f\"Attention map saved to: {out_path}\")\n",
        "    else:\n",
        "        print(\"matplotlib not available: skipping attention visualization.\")\n",
        "\n",
        "    print(\"Demo forward pass complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "hbdIWj9Ohw6d",
        "outputId": "aa1986af-7ad3-43c8-a64e-afd972892b75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token ids shape: (2, 10)\n",
            "Logits shape: (2, 10, 1000)\n",
            "Probabilities for last token shape: (2, 1000)\n",
            "Sample logits (batch0, last pos, first 10): [-0.66862187 -0.01557043 -0.31070113  0.30578152  0.67496351 -1.51740577\n",
            " -2.00944891 -1.56804171  0.25936107  0.2532878 ]\n",
            "Sum of prob distribution (batch0, last pos): 0.9999999999999999\n",
            "Last-attention shape: (2, 4, 10, 10)\n",
            "Top-5 next-token predictions (batch0, last pos): [819 877 974 194 391]\n",
            "Attention row for position 2 (example): [0.094825 0.322204 0.582971 0.       0.       0.       0.       0.\n",
            " 0.       0.      ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention map saved to: attention_map.png\n",
            "Demo forward pass complete.\n"
          ]
        }
      ]
    }
  ]
}